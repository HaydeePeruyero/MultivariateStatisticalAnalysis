# Regresión múltiple

```{r setup1, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
library(broom) # para resúmenes ordenados de modelos
```

## ¿Por qué estadística multivariada?

El proceso de modelado consiste en construir expresiones matemáticas que permitan representar el comportamiento de una variable que queremos estudiar. Cuando contamos con varias variables, suele interesarnos analizar cómo unas influyen sobre otras, determinando si existe una relación, su intensidad y su forma. En muchos casos, estas relaciones pueden ser complejas y difíciles de describir directamente; por ello, se busca aproximarlas mediante funciones matemáticas sencillas como polinomios, que conserven los elementos esenciales para explicar el fenómeno de interés.

Cuando estudiamos fenómenos deterministas, es común vincular una variable dependiente con una o más variables independientes. Por ejemplo, en la ecuación de la velocidad ($v=d/t$), la distancia depende de la velocidad y del tiempo. En la práctica, cuando realizamos distintos experimentos, las fórmulas deterministas podrían no capturar por completo el comportamiento observado. Esto puede deberse a factores no controlados, a la presencia de variabilidad natural o a efectos aleatorios. Por esta razón, además de la parte determinista del modelo, se incorpora un término que represente la discrepancia aleatoria entre lo que se predice y lo que efectivamente se observa. De forma general, esta idea se resume como:

$$Observación = Modelo \ + \ Error$$

Cuando se supone que la relación entre las variables puede representarse mediante una ecuación lineal, hablamos de *análisis de regresión lineal*. Si intervienen únicamente dos variables, una dependiente $y$ y independiente $x$, se trata de **regresión lineal simple**. En cambio, cuando la variable de interés $y$ depende de dos o más variables independientes $x_1,x_2, ...$ hablamos de **regresión lineal múltiple**. 


*Supongamos que queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia?*

En este caso se tiene que el puntaje o rendimiento lo podemos representar con $y$ y las horas de estudio con $x$. Entonces esta propuesta de modelo, la podríamos representar como:

$$y=\beta_0+\beta_1x$$
Donde $\beta_0$ es la ordenada al origen y $\beta_1$ la pendiente. Esta recta podría no ajustarse al modelo por diferentes razones, entonces lo que se hace es considerar un error aleatorio $\epsilon$. El modelo que ya considera este error se representa como:

$$y=\beta_0+\beta_1x+\epsilon.$$

A este modelo se le conoce como modelo de **regresión lineal simple** y a $\beta_0,\beta_1$ se les conoce como **coeficientes de regresión**.

En problemas reales, casi nunca una sola variable explica el fenómeno. Las decisiones y predicciones mejoran cuando integramos múltiples fuentes de información.

Ejemplos:
- Salud: riesgo de una enfermedad según edad, IMC, actividad física, dieta y antecedentes.
- Ingeniería: vida útil de una pieza según temperatura, vibración, material y carga.
- Biología: crecimiento de una planta por agua, luz, fertilizante, temperatura.


**Ejemplo:** Si queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia? ¿qué otras variables podrían influir en el puntaje de un examen?

<details> <summary><b> Rendimiento escolar </b></summary>
```{r}
set.seed(123)
n <- 10
data_intro <- tibble(
  estudiante = paste0("E", 1:n),
  horas_estudio = c(2,3,4,5,1,3,2,4,5,6),
  horas_sueno  = c(7,8,6,7,5,8,7,6,9,7),
  asistencia   = c(0.9,0.95,0.8,0.85,0.7,0.9,0.8,0.9,1,0.95),
  puntaje      = c(65,70,68,80,60,75,65,78,88,85)
)
data_intro
```
</details>


¿Qué pasa si solo graficamos horas de estudio vs puntaje?

<details> <summary><b> Plot hotas de estudio vs puntaje sugerida</b></summary>
```{r}
library(ggplot2)
ggplot(data_intro, aes(horas_estudio, puntaje)) +
  geom_point(size=3) +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="¿Solo horas de estudio explican el puntaje?")
```

</details>


¿Se ajusta un modelo lineal? ¿Porqué?


### ¿Qué es “multivariado” y por qué lo necesitamos?


**Idea central:** cuando **varias** $x$ influyen sobre $y$, estudiar cada $x$ por separado puede engañarnos. El análisis multivariado permite:

- **Aislar efectos**: estimar el efecto de $x_1$ *manteniendo constantes* $x_2,x_3,...$.
- **Mejorar predicción**: reducir error al añadir información relevante.
- **Controlar confusores**: variables que cambian la relación aparente entre $y$ y $x$.


**Ejemplo:** Si ajustamos ahora un modelo con varias variables, ¿vamos a observar un cambio? ¿se ajustará mejor?

<details>
<summary><b>Código (modelos + comparaciones)</b></summary>

```{r}
# Modelo simple
m1 <- lm(puntaje ~ horas_estudio, data = data_intro)

# Modelo múltiple
m2 <- lm(puntaje ~ horas_estudio + horas_sueno + asistencia, data = data_intro)

# Medidas clave
R2_m1  <- glance(m1)$r.squared
R2_m2  <- glance(m2)$r.squared

print(paste("El R2 del modelo simple:", R2_m1))
print(paste("El R2 del modelo multiple:", R2_m2))

#R2adj_m1 <- glance(m1)$adj.r.squared
#R2adj _m2 <- glance(m2)$adj.r.squared
```


- ¿Aumentó $R^2$ al incluir más variables? ¿Por qué tiende a subir?
- ¿Qué cambia en la interpretación de horas_estudio al controlar por horas_sueno y asistencia?
- ¿Puede un predictor ser importante en bivariado y no en multivariado (o viceversa)?


## Regresión múltiple



### Modelo y estimación

Los modelos en regresión lineal múltiple están dados por la siguiente forma, donde $y$ depende de $p$ variables predictoras:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2} + \beta_px_{ip}+\epsilon_i.$$

Se suele asumir que los errores $\epsilon_i$ son i.i.d. con distribución normal de media 0 y varianza $\sigma^2$ desconocida. Los coeficientes $\beta_i$ son constantes desconocidas y son los parámetros del modelo. Cada $\beta_j$ representa el cambio esperado en la respuesta $y$ por el cambio unitario en $x_i$ cuando todas las demás variables independientes $x_i(i\neq j)$ se mantienen constantes. 

```{r, eval=FALSE}
# Forma general
ajuste <- lm(y ~ x1 + x2 + ... + xp, data = datos)
# summary(ajuste)
```

Los coeficientes los podemos interpretar como sigue:

- **Intercepto ($\beta_0$)**: valor esperado de $y$ cuando todas las $x$=0.
- **Pendiente $\beta_j$**: efecto **parcial** de $x_j$ sobre $y$ manteniendo las demás constantes.



En los modelos de regreción lineal, solemos usar las siguientes medidas de bondad de ajuste:

- **$R^2$**: proporción de varianza de $y$ explicada.
- **$R^2$ ajustado**: penaliza por número de predictores (mejor para comparar modelos con distinto número de x).
- **RMSE ($\sigma$)**: error típico de predicción en unidades de $y$.

```{r}
comp <- dplyr::bind_rows(
  glance(m1) %>% mutate(modelo="simple"),
  glance(m2) %>% mutate(modelo="multiple")
) %>% select(modelo, r.squared, adj.r.squared)
comp
```

Para este modelo algunos de los supuestos se siguen del modelo de regresión lineal
simple y se agregan algunos que tienen que ver con la relación que pudiera existir entre
las variables regresoras.


- El modelo es lineal en los parámetros.  
  *Chequeo*: residuales vs ajustados sin patrón claro.
- El modelo está especificado correctamente.
- Covarianza cero entre variables regresoras y el error.
- Esperanza del error igual a cero.
- Homocedasticidad.
- No autocorrelación entre los errores.
- Los errores siguen una distribución normal.
- Mas observaciones que parámetros a estimar.
- Variación entre los valores de las variables regresoras.
- No colinealidad (multicolinealidad) entre las variables regresoras, es decir, no existe
una relación lineal entre $x_i$ y $x_j$ (es decir, las variables son linealmente independientes).

<details>
<summary><b>Supuestos</b></summary>

```{r}
# Modelo m2
par(mfrow=c(1,2))
plot(m2, which=1)  # Residuales vs ajustados
plot(m2, which=2)  # QQ-plot
```

</details>

---

**Ejercicio**: Supongamos que tenemos los siguientes datos: precio de vivienda según metros, habitaciones y distancia al centro.

<details>
<summary><b>Dataset</b></summary>

```{r}
set.seed(42)
n <- 14
casas <- tibble::tibble(
  precio = c(200,220,250,275,300,180,210,260,280,320,190,240,230,305),
  metros = c(80,90,100,110,120,70,85,105,115,130,75,95,92,125),
  habitaciones = c(2,3,3,4,4,2,3,3,4,5,2,3,3,4),
  distancia_centro = c(5,4,6,3,2,8,6,3,2,1,7,5,4,2)
)
casas
```

</details>


1) Ajusta `precio ~ metros` (simple) y `precio ~ metros + habitaciones + distancia_centro` (múltiple).  
2) Compara $R^2$,  $R^2$ **ajustado** y **σ (RMSE)**.  
3) Interpreta el coeficiente de `distancia_centro`.  
4) Revisa QQ-plot y residuales vs ajustados. ¿Algún patrón?  

<details>
<summary><b>Solución</b></summary>

```{r}
m_s <- lm(precio ~ metros, data=casas)
m_m <- lm(precio ~ metros + habitaciones + distancia_centro, data=casas)

broom::glance(m_s)[,c("r.squared","adj.r.squared")]
broom::glance(m_m)[,c("r.squared","adj.r.squared")]
broom::tidy(m_m)

par(mfrow=c(1,2))
plot(m_m, which=1)
plot(m_m, which=2)
```

</details>


## Estimación de parámetros


**Ejemplo (Montgomery, 2002):** : Un embotellador de bebidas gaseosas analiza las rutas de servicio de las máquinas expendedoras en su sistema de distribución. Le interesa predecir el tiempo necesario para que el representante de ruta atienda las máquinas expendedoras en una tienda. 

Esta actividad de servicio consiste en abastecer la máquina con productos embotellados, y algo de mantenimiento o limpieza. El ingeniero industrial responsable del estudio ha sugerido que las dos variables más importantes que afectan el tiempo de entrega \(y\) son la cantidad de cajas de producto abastecido, \(x_1\), y la distancia caminada por el representante, \(x_2\). 

El ingeniero ha reunido 25 observaciones de tiempo de entrega que se ven en la tabla siguiente. Se ajustará el modelo de regresión lineal multiple siguiente:

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +  \varepsilon
\]

*Archivo: `refrescos.csv`*.

<details>
<summary><b>Base de datos</b></summary>
```{r}
# 
datos <- data.frame(
  Observacion = 1:25,
  y = c(16.68, 11.50, 12.03, 14.88, 13.75,
        18.11, 8.00, 17.83, 79.24, 21.50,
        40.33, 21.00, 13.50, 19.75, 24.00,
        29.00, 15.35, 19.00, 9.50, 35.10,
        17.90, 52.32, 18.75, 19.83, 10.75),
  x1 = c(7, 3, 3, 4, 6,
         7, 2, 7, 30, 5,
         16, 10, 4, 6, 9,
         10, 6, 7, 3, 17,
         10, 26, 9, 8, 4),
  x2 = c(560, 220, 340, 80, 150,
         330, 110, 210, 1460, 605,
         688, 215, 255, 462, 448,
         776, 200, 132, 36, 770,
         140, 810, 450, 635, 150)
)

datos
```
</details>


Veamos un gráfico de dispersión de los datos. ¿Qué observamos?

```{r}
pairs(datos[-1])
```


1) Estimar $\beta$

Primero, vamos a crear la matriz $X$ y el vector $y$.

<details>
<summary><b>Matrices</b></summary>
```{r}
# Columna de 1 para el intercepto
idv <- rep(1, nrow(datos))
# Creamos matriz X
X <- matrix(c(idv,datos$x1,datos$x2),nrow=25,ncol=3)
# Creamos el vector y
y <- matrix(datos$y, nrow = 25, ncol = 1)
```
</details>

Ya sabemos que nuestro estimador está dado por 

\[
\hat{\beta} = (X'X)^{-1}X'y
\]

Entonces podemos encontrar el estimador.

<details>
<summary><b>Estimador beta</b></summary>
```{r}
beta <- solve(t(X) %*% X) %*% t(X) %*% y
beta
```
</details>

Entonces el ajuste por el método de mínimos cuadrados, con los coeficientes de regresión que encontramos está dado por:

$\hat{y} =$ `r beta[1]` $+$ `r beta[2]` $x_1$ $+$ `r beta[2]` $x_2$


Esto lo podemos hacer más rápido usando la función de `lm`. Construimos el modelo.

<details>
<summary><b>Modelo en R</b></summary>
```{r}
M1 <- lm(y ~ x1 + x2, datos)
M1 
```
</details>


¿Cómo accedemos a los valores del modelo?

<details>
<summary><b>Coeficientes</b></summary>
```{r}
beta_0 <- M1$coefficients[1]
beta_1 <- M1$coefficients[2]
beta_2 <- M1$coefficients[3]
```
</details>

Los valores son $\beta_0=$ `r beta_0`, $\beta_1=$ `r beta_1` y $\beta_2=$ `r beta_2`.


2) Estimación de la varianza del error $\sigma^2$

Ya tenemos que la suma de los cuadrados de los errores está dada por

\[ SSE = y'y - \hat{\beta}X'y\]

Sustituimos los valores que tenemos y obtemos el SSE.

<details>
<summary><b>SSE</b></summary>
```{r}
SSE <- t(y)%*% y - t(beta) %*% t(X) %*% y
SSE
```
</details>

Y de está forma, podemos encontrar el estimador de $\sigma^2$. 

<details>
<summary><b>Estimador</b></summary>
```{r}
varest <- SSE / (nrow(y) - nrow(beta))
varest
```
</details>

Directo con las funciones de R, podemos acceder a los parámetros que se guardaron en el modelo que ya calculamos.

<details>
<summary><b>Resumen del modelo</b></summary>
```{r}
summary(M1)
```
</details>

Algunos de los parámetros almacenados en el modelo nos permiten obtener también el resultado previo. 

<details>
<summary><b>Estimador</b></summary>
```{r}
sum(residuals(M1)^2) / df.residual(M1)
```
</details>


### Ejercicios

**Ejercicio 1:** Un analista hace un estudio químico y espera que el rendimiento de cierta sustancia se vea afectado por dos factores. Se realizan 17 experimentos cuyos datos se registran en el cuadro siguiente. Por experimentos similares, se sabe que los factores \(x_1\) y \(x_2\) no están relacionados; por ello, el analista decide utilizar un modelo de regresión lineal múltiple. Calcule el modelo de regresión y grafíquelo sobre las observaciones. 

*Archivo: est_quimico.csv*

<details>
<summary><b>Datos Ejercicio 1</b></summary>
```{r}
datos2 <- data.frame(
  Experimento = 1:17,
  x1 = c(41.9, 43.4, 43.9, 44.5, 47.3, 47.5, 47.9, 50.2, 52.8, 53.2, 56.7, 57.0, 63.5, 64.3, 71.1, 77.0, 77.8),
  x2 = c(29.1, 29.3, 29.5, 29.7, 29.9, 30.3, 30.5, 30.7, 30.8, 30.9, 31.5, 31.7, 31.9, 32.0, 32.1, 32.5, 32.9),
  y  = c(251.3, 251.3, 248.3, 267.5, 273.0, 276.5, 270.3, 274.9, 285.0, 290.0, 297.0, 302.5, 304.5, 309.3, 321.7, 330.7, 349.0)
)

datos2
```
</details>

**Ejercicio 2:** Repetir el ejemplo con los datos `datasets::trees` de R que proporciona mediciones
del diámetro, altura y volumen de madera en 31 cerezos negros talados.


**Ejercicio 3:** Subir a Github los dos ejercicios previos tanto con solución en R como en Python. Comparar las funciones. Ventajas y desventajas de ambas.


## Pruebas de Hipótesis

Cuando revisamos el `summary` del modelo, nos arroja si son significativas o no y a que nivel de significancia las variables que estamos considerando. Veamos el siguiente ejemplo.

### Prueba de la significancia de la regresión


**Ejemplo:** Con los datos del embotellador de bebidas gaseosas, se probará la significancia de la regresión.

<details>
<summary><b>Sumas de Cuadrados</b></summary>
```{r}
SCT <- t(y) %*% y -  sum(y)**2 / nrow(datos)
SCT

SCE <- t(beta) %*% t(X) %*% y - sum(y)**2 / nrow(datos)
SCE

SSE <- SCT - SCE
SSE
                                      
```
</details>

Para probar 

\[H_0 : \beta_1 = \beta_2=0\]

se calcula el estadístico:

<details>
<summary><b>Estadístico F</b></summary>
```{r}
F0 <- (SCE / (ncol(X) - 1)) / (SSE / (nrow(X) - (ncol(X) -1) - 1))
F0
```
</details>

Como el valor de $F_0$ es mayor que el valor tabulado de $F_{\alpha;p,n-p-1}=F_{0.05;2;22}=3.44$, se rechaza $H_0$. Lo cual implica qye el tiempo de entrega depende del volumen de entrega y/o de la distancia.


Ahora, usando los modelos que ya calculamos.

<details>
<summary><b>Sumas de cuadrados</b></summary>
```{r}
SCT.m<-sum((datos$y-mean(datos$y))^2)
SCT.m

SCE.m <-sum((M1$fitted-mean(datos$y))^2)
SCE.m

SSE.m <-sum(M1$residuals^2)
SSE.m
```
</details>


<details>
<summary><b>Grados de libertad</b></summary>
```{r}
n<-nrow(y)
n

GLT<- n-1
GLT


GLRes<- df.residual(M1)
GLRes

GLR<- GLT-GLRes
GLR
```
</details>

<details>
<summary><b>Cuadrados medios</b></summary>
```{r}
CMR <- SCE /GLR
CMR

CMRes <- SSE / GLRes
CMRes
```
</details>

<details>
<summary><b>Estadístico F_0</b></summary>
```{r}
F0 <- CMR/CMRes
F0
```
</details>

<details>
<summary><b>p-valor</b></summary>
```{r}
pv <- 1 - pf(F0, GLR,GLRes)
pv
```
</details>

<details>
<summary><b>Valor tabulado de F</b></summary>
```{r}
alpha <- 0.05; df1 <- 2; df2 <- 22
F_crit <- qf(1 - alpha, df1, df2)
F_crit
```
</details>


### Pruebas sobre coeficientes individuales de regresión


**Ejemplo:** Usando los datos del embotellador de bebidas gaseosas, se desea evaluar la importancia de la variable regresora *distancia* ($x_2$) dado que el regresor *cajas* ($x_1$) está en el modelo.

<details>
<summary><b>Estadístico t_0</b></summary>
```{r}
C22 <- solve(t(X) %*% X)[3,3]
C22

t0 <- beta_2 / sqrt(varest * C22)
t0

## t tabulado con confianza 95% y 22 grados de libertad
tt <- qt(p = 0.95 + 0.05/2, df = 22, lower.tail = TRUE)
tt
```
</details>


Usando el modelo que ya tenemos calculado `M1` podemos obtener estos mismos resultados de la siguiente forma.

<details>
<summary><b>Prueba sobre coeficientes</b></summary>
```{r}
summary(M1)
```
</details>

## Intervalos de confianza

### Intervalos de confianza en los coeficientes de regresión

**Ejemplo:** Usando los datos del embotellador de bebidas gaseosas, queremos calcular el intervalo de confianza del 95\% para $\beta_1$. Recordemos que el estimador puntual de $\beta_1$ es `r beta_1`.

<details>
<summary><b>Intervalo de confianza</b></summary>
```{r}
C11 <- solve(t(X) %*% X)[2,2]

izq <- beta_1 - tt * sqrt(varest*C11)
izq

der <- beta_1 + tt * sqrt(varest*C11)
der
```
</details>


### Intervalo de confianza de la respuesta media


**Ejemplo:** El embotellador de bebidas gaseosas quiere establecer un intervalo de confianza del 95\% para el tiempo medio de entrega para una tienda donde se requieren $x_1=8$ cajas y la distancia es de $x_2=275$ pies. 

Nuestro vector $X_0$ está dado por:
<details>
<summary><b>X0</b></summary>
```{r}
X0 <- matrix(c(1, 8, 275), nrow = 3)
X0
```
</details>


El valor ajustado en ese punto es:


<details>
<summary><b>Valor ajustado</b></summary>
```{r}
y0 <- t(X0) %*% beta
y0
```
</details>

La varianza de $\hat{y_0}$

<details>
<summary><b>Varianza</b></summary>
```{r}
var_y0 <- varest * t(X0) %*% solve(t(X) %*% X) %*% X0
var_y0
```
</details>

Entonces el intervalo de confianza en este punto es:

<details>
<summary><b>Intervalo de confianza</b></summary>
```{r}
l_izq <- y0 - tt * sqrt(var_y0) 
l_izq

l_der <- y0 + tt * sqrt(var_y0) 
l_der
```
</details>



**Ejemplo: ** Usaremos el conjunto de datos `data("marketing")` que contiene 200 observaciones de un experimento publicitario que evalúa el impacto de tres medios de anuncio en las ventas. Para cada observación se registran los presupuestos de publicidad (en miles de dólares) y las ventas obtenidas.
Variables:

- `youtube`: presupuesto invertido en anuncios de YouTube (miles de USD).
- `facebook`: presupuesto invertido en Facebook (miles de USD).
- `newspaper`: presupuesto invertido en prensa escrita (miles de USD).
- `sales`: ventas registradas (variable respuesta).

Cargamos los datos:

```{r}
library(datarium)
data("marketing")
```

Exploramos rápidamente la base para ver qué variables contiene y la dimensión:
```{r}
str(marketing)
#?marketing
```

Ajustamos un modelo lineal que incluya todas las variables, es decir, 

$sales=\beta_0+\beta_1 youtube+\beta_2 facebook+ \beta_3 newspaper+\epsilon$

<details>
<summary><b>Modelo marketing</b></summary>
```{r}
modelo1<-lm(sales~youtube+facebook+newspaper,data=marketing)
summary(modelo1)
```
</details>

¿Qué se puede decir sobre la significancia de la variable $newspaper$?

Veamos qué ocurre con el modelo al eliminar la variable $newspaper$

<details>
<summary><b>Modelo marketing 2</b></summary>
```{r}
modelo2<-lm(sales~facebook+youtube,data=marketing)
summary(modelo2)
``` 
</details>

Lo que sigue, es hacer pruebas de hipótesis tanto en las variables como en los coeficientes de regresión.




**Ejercicio 1:** Realizar las pruebas de hipótesis sobre la significancia de la regresión y sobre los coeficientes. Encontrar los intervalos de confianza respectivos del 95\%. 
Para una tienda con presupuestos:
$youtube = 150$, $facebook = 30$, $newspaper=20$ (en miles de USD):
(a) Calcula el intervalo de confianza del 95% para la media de ventas $\mathbb{E}(sales | X_0)$.
(b) Calcula el intervalo de predicción del 95% para una nueva observación de ventas.
(c) Comenta la diferencia entre ambos intervalos.
Subir respuesta y explicación de sus resultados a github. 


## Ejercicios 

**Ejercicio 1:** Para los datos de la Liga Nacional de Fútbol

a) Ajustar un modelo de regresión lineal múltiple que relacione la cantidad de juegos ganados con las yardas por aire del equipo ($x_2$), el porcentaje de jugadas por tierra ($x_7$) y las yardas por tierra del contrario ($x_8$).  

b) Formar la tabla de análisis de varianza y probar la significancia de la regresión.  

c) Calcular el estadístico *t* para probar las hipótesis $H_0 : \beta_2 = 0$, $H_0 : \beta_7 = 0$ y $H_0 : \beta_8 = 0$. ¿Qué conclusiones se pueden sacar acerca del papel de las variables $x_2$, $x_7$ y $x_8$ en el modelo?  

d) Calcular $R^2$ y $R^2_{adj}$ para este modelo.  

e) Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?  

f) Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.  

g) Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?  

h) Calcular un intervalo de confianza de $95\%$ para $\beta_7$ y un intervalo de confianza de $95\%$ para la cantidad media de juegos ganados por un equipo cuando $x_2 = 2300$, $x_7 = 56$ y $x_8 = 2100$.  

i) Ajustar un modelo a esos datos, usando solo $x_7$ y $x_8$ como regresores y probar la significancia de la regresión.  

j) Calcular $R^2$ y $R^2_{adj}$. Compararlos con los resultados del modelo anterior.  

k) Calcular un intervalo de confianza de $95\%$ para $\beta_7$. También, un intervalo de confianza de $95\%$ para la cantidad media de juegos ganados por un equipo cuando $x_7 = 56$ y $x_8 = 2100$. Comparar las longitudes de esos intervalos de confianza con las longitudes de los correspondientes al modelo anterior.  

l) ¿Qué conclusiones se pueden sacar de este problema, acerca de las consecuencias de omitir un regresor importante de un modelo?  


**Ejericio 2:** Véase los datos de rendimiento de gasolina.

a) Ajustar un modelo de regresión lineal múltiple que relacione el rendimiento de la gasolina *y*, en millas por galón, la cilindrada del motor ($x_1$) y la cantidad de gargantas del carburador ($x_6$).  

b) Formar la tabla de análisis de varianza y probar la significancia de la regresión.  

c) Calcular $R^2$ y $R^2_{adj}$ para este modelo. Compararlas con las $R^2$ y $R^2_{adj}$ Ajustado para el modelo de regresión lineal simple, que relaciona las millas con la cilindrada.  

d) Determinar un intervalo de confianza para $\beta_1$.  

e) Determinar un intervalo de confianza de $95\%$ para el rendimiento promedio de la gasolina, cuando $x_1 = 225 pulg^3$ y $x_6 = 2$ gargantas.  

f) Determinar un intervalo de predicción de $95\%$ para una nueva observación de rendimiento de gasolina, cuando $x_1 = 225 pulg^3$ y $x_6 = 2$ gargantas.  

g) Considerar el modelo de regresión lineal simple, que relaciona las millas con la cilindrada. Construir un intervalo de confianza de $95\%$ para el rendimiento promedio de la gasolina y un intervalo de predicción para el rendimiento, cuando $x_1 = 225 pulg^3$. Comparar las longitudes de estos intervalos con los intervalos obtenidos en los dos incisos anteriores. ¿Tiene ventajas agregar $x_6$ al modelo?  

h) Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?  

i) Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.  

j) Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?  




