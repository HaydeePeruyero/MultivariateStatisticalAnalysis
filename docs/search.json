[{"path":"index.html","id":"estadística-multivariada","chapter":"Sección 1 Estadística Multivariada","heading":"Sección 1 Estadística Multivariada","text":"","code":""},{"path":"index.html","id":"temario","chapter":"Sección 1 Estadística Multivariada","heading":"1.1 Temario","text":"Regresión múltiple1.1 Mínimos cuadrados.1.2 Medidas de bondad de ajuste.1.3 Determinación del número de variables predictorias.Análisis de componentes principales2.1 Descripción de la metodología.2.2 Técnicas de extracción de componentes principales.2.3 Determinación del número de componentes principales.Análisis factorial3.1 Descripción de la metodología del análisis factorial.3.2 Descripción del modelo básico.3.3 Método de cálculo.3.4 Comparación con la técnica del análisis de componentes principales.3.5 Usos de software (R, Minitab, SciPy, entre otros).Análisis de conglomerados4.1Descripción de la metodología de análisis de conglomerados.4.2 Técnicas de jerarquización y de particionamiento.4.3 Implementación computacional.4.4 Usos de los dendogramas.4.5 Usos de software (R, Minitab, SciPy, entre otros).Análisis discriminante5.1 Descripción de la metodología del análisis discriminante.5.2 Discriminación entre dos grupos.5.3 Contribución por variable.5.4 Discriminación logística.5.5 Discriminación múltiple.5.6 Usos de software (R, Minitab, SciPy, entre otros).A1. RA2. Git + GithubA3. Gráficas MultivariadasA4. Escalas de MediciónA5. Valores Faltantes","code":""},{"path":"index.html","id":"evaluación","chapter":"Sección 1 Estadística Multivariada","heading":"1.2 Evaluación","text":"Examenes 50%Tareas 25%Proyecto 20%DataCamp 5%","code":""},{"path":"index.html","id":"proyecto-final","chapter":"Sección 1 Estadística Multivariada","heading":"1.3 Proyecto final","text":"Buscar una base de datos “real”Aplicar 3 métodos de estadística multivariadaEntregar documento con:\nDescripción de los datos\nPlanteamiento del problema\nMétodos usados\nInterpretación de resultados\nCódigo usado\nDescripción de los datosPlanteamiento del problemaMétodos usadosInterpretación de resultadosCódigo usadoRepositorio con código reproducibleExposición de resultados","code":""},{"path":"index.html","id":"referencias","chapter":"Sección 1 Estadística Multivariada","heading":"1.4 Referencias","text":"[1]","code":""},{"path":"index.html","id":"material-interesante","chapter":"Sección 1 Estadística Multivariada","heading":"1.5 Material interesante","text":"Bookdown.Software Carpentry.GitWhy GitR Markdown CookbookSTHDAYaRrr! Pirate’s Guide RLearn ggplot2 Using Shiny AppGgplot2: Elegant Graphics Data Analysis\nVersión online\nVersión onlineUse R! Colección SpringerLattice: Multivariate Data Visualization RR Graphics cookbookCuenta pro de Github","code":""},{"path":"index.html","id":"datacamp","chapter":"Sección 1 Estadística Multivariada","heading":"1.6 DataCamp","text":"","code":""},{"path":"regresión-múltiple.html","id":"regresión-múltiple","chapter":"Sección 2 Regresión múltiple","heading":"Sección 2 Regresión múltiple","text":"","code":""},{"path":"regresión-múltiple.html","id":"por-qué-estadística-multivariada","chapter":"Sección 2 Regresión múltiple","heading":"2.1 ¿Por qué estadística multivariada?","text":"El proceso de modelado consiste en construir expresiones matemáticas que permitan representar el comportamiento de una variable que queremos estudiar. Cuando contamos con varias variables, suele interesarnos analizar cómo unas influyen sobre otras, determinando si existe una relación, su intensidad y su forma. En muchos casos, estas relaciones pueden ser complejas y difíciles de describir directamente; por ello, se busca aproximarlas mediante funciones matemáticas sencillas como polinomios, que conserven los elementos esenciales para explicar el fenómeno de interés.Cuando estudiamos fenómenos deterministas, es común vincular una variable dependiente con una o más variables independientes. Por ejemplo, en la ecuación de la velocidad (\\(v=d/t\\)), la distancia depende de la velocidad y del tiempo. En la práctica, cuando realizamos distintos experimentos, las fórmulas deterministas podrían capturar por completo el comportamiento observado. Esto puede deberse factores controlados, la presencia de variabilidad natural o efectos aleatorios. Por esta razón, además de la parte determinista del modelo, se incorpora un término que represente la discrepancia aleatoria entre lo que se predice y lo que efectivamente se observa. De forma general, esta idea se resume como:\\[Observación = Modelo \\ + \\ Error\\]Cuando se supone que la relación entre las variables puede representarse mediante una ecuación lineal, hablamos de análisis de regresión lineal. Si intervienen únicamente dos variables, una dependiente \\(y\\) y independiente \\(x\\), se trata de regresión lineal simple. En cambio, cuando la variable de interés \\(y\\) depende de dos o más variables independientes \\(x_1,x_2, ...\\) hablamos de regresión lineal múltiple.Supongamos que queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia?En este caso se tiene que el puntaje o rendimiento lo podemos representar con \\(y\\) y las horas de estudio con \\(x\\). Entonces esta propuesta de modelo, la podríamos representar como:\\[y=\\beta_0+\\beta_1x\\]\nDonde \\(\\beta_0\\) es la ordenada al origen y \\(\\beta_1\\) la pendiente. Esta recta podría ajustarse al modelo por diferentes razones, entonces lo que se hace es considerar un error aleatorio \\(\\epsilon\\). El modelo que ya considera este error se representa como:\\[y=\\beta_0+\\beta_1x+\\epsilon.\\]este modelo se le conoce como modelo de regresión lineal simple y \\(\\beta_0,\\beta_1\\) se les conoce como coeficientes de regresión.En problemas reales, casi nunca una sola variable explica el fenómeno. Las decisiones y predicciones mejoran cuando integramos múltiples fuentes de información.Ejemplos:\n- Salud: riesgo de una enfermedad según edad, IMC, actividad física, dieta y antecedentes.\n- Ingeniería: vida útil de una pieza según temperatura, vibración, material y carga.\n- Biología: crecimiento de una planta por agua, luz, fertilizante, temperatura.Ejemplo: Si queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia? ¿qué otras variables podrían influir en el puntaje de un examen?¿Qué pasa si solo graficamos horas de estudio vs puntaje?¿Se ajusta un modelo lineal? ¿Porqué?","code":"\nset.seed(123)\nn <- 10\ndata_intro <- tibble(\n  estudiante = paste0(\"E\", 1:n),\n  horas_estudio = c(2,3,4,5,1,3,2,4,5,6),\n  horas_sueno  = c(7,8,6,7,5,8,7,6,9,7),\n  asistencia   = c(0.9,0.95,0.8,0.85,0.7,0.9,0.8,0.9,1,0.95),\n  puntaje      = c(65,70,68,80,60,75,65,78,88,85)\n)\ndata_intro## # A tibble: 10 × 5\n##    estudiante horas_estudio horas_sueno asistencia puntaje\n##    <chr>              <dbl>       <dbl>      <dbl>   <dbl>\n##  1 E1                     2           7       0.9       65\n##  2 E2                     3           8       0.95      70\n##  3 E3                     4           6       0.8       68\n##  4 E4                     5           7       0.85      80\n##  5 E5                     1           5       0.7       60\n##  6 E6                     3           8       0.9       75\n##  7 E7                     2           7       0.8       65\n##  8 E8                     4           6       0.9       78\n##  9 E9                     5           9       1         88\n## 10 E10                    6           7       0.95      85\nlibrary(ggplot2)\nggplot(data_intro, aes(horas_estudio, puntaje)) +\n  geom_point(size=3) +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(title=\"¿Solo horas de estudio explican el puntaje?\")## `geom_smooth()` using formula = 'y ~ x'"},{"path":"regresión-múltiple.html","id":"qué-es-multivariado-y-por-qué-lo-necesitamos","chapter":"Sección 2 Regresión múltiple","heading":"2.1.1 ¿Qué es “multivariado” y por qué lo necesitamos?","text":"Idea central: cuando varias \\(x\\) influyen sobre \\(y\\), estudiar cada \\(x\\) por separado puede engañarnos. El análisis multivariado permite:Aislar efectos: estimar el efecto de \\(x_1\\) manteniendo constantes \\(x_2,x_3,...\\).Mejorar predicción: reducir error al añadir información relevante.Controlar confusores: variables que cambian la relación aparente entre \\(y\\) y \\(x\\).Ejemplo: Si ajustamos ahora un modelo con varias variables, ¿vamos observar un cambio? ¿se ajustará mejor?¿Aumentó \\(R^2\\) al incluir más variables? ¿Por qué tiende subir?¿Qué cambia en la interpretación de horas_estudio al controlar por horas_sueno y asistencia?¿Puede un predictor ser importante en bivariado y en multivariado (o viceversa)?","code":"\n# Modelo simple\nm1 <- lm(puntaje ~ horas_estudio, data = data_intro)\n\n# Modelo múltiple\nm2 <- lm(puntaje ~ horas_estudio + horas_sueno + asistencia, data = data_intro)\n\n# Medidas clave\nR2_m1  <- glance(m1)$r.squared\nR2_m2  <- glance(m2)$r.squared\n\nprint(paste(\"El R2 del modelo simple:\", R2_m1))## [1] \"El R2 del modelo simple: 0.824317362184441\"\nprint(paste(\"El R2 del modelo multiple:\", R2_m2))## [1] \"El R2 del modelo multiple: 0.895428180549875\"\n#R2adj_m1 <- glance(m1)$adj.r.squared\n#R2adj _m2 <- glance(m2)$adj.r.squared"},{"path":"regresión-múltiple.html","id":"regresión-múltiple-1","chapter":"Sección 2 Regresión múltiple","heading":"2.2 Regresión múltiple","text":"","code":""},{"path":"regresión-múltiple.html","id":"modelo-y-estimación","chapter":"Sección 2 Regresión múltiple","heading":"2.2.1 Modelo y estimación","text":"Los modelos en regresión lineal múltiple están dados por la siguiente forma, donde \\(y\\) depende de \\(p\\) variables predictoras:\\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2} + \\beta_px_{ip}+\\epsilon_i.\\]Se suele asumir que los errores \\(\\epsilon_i\\) son ..d. con distribución normal de media 0 y varianza \\(\\sigma^2\\) desconocida. Los coeficientes \\(\\beta_i\\) son constantes desconocidas y son los parámetros del modelo. Cada \\(\\beta_j\\) representa el cambio esperado en la respuesta \\(y\\) por el cambio unitario en \\(x_i\\) cuando todas las demás variables independientes \\(x_i(\\neq j)\\) se mantienen constantes.Los coeficientes los podemos interpretar como sigue:Intercepto (\\(\\beta_0\\)): valor esperado de \\(y\\) cuando todas las \\(x\\)=0.Pendiente \\(\\beta_j\\): efecto parcial de \\(x_j\\) sobre \\(y\\) manteniendo las demás constantes.En los modelos de regreción lineal, solemos usar las siguientes medidas de bondad de ajuste:\\(R^2\\): proporción de varianza de \\(y\\) explicada.\\(R^2\\) ajustado: penaliza por número de predictores (mejor para comparar modelos con distinto número de x).RMSE (\\(\\sigma\\)): error típico de predicción en unidades de \\(y\\).Para este modelo algunos de los supuestos se siguen del modelo de regresión lineal\nsimple y se agregan algunos que tienen que ver con la relación que pudiera existir entre\nlas variables regresoras.El modelo es lineal en los parámetros.Chequeo: residuales vs ajustados sin patrón claro.El modelo está especificado correctamente.Covarianza cero entre variables regresoras y el error.Esperanza del error igual cero.Homocedasticidad.autocorrelación entre los errores.Los errores siguen una distribución normal.Mas observaciones que parámetros estimar.Variación entre los valores de las variables regresoras.colinealidad (multicolinealidad) entre las variables regresoras, es decir, existe\nuna relación lineal entre \\(x_i\\) y \\(x_j\\) (es decir, las variables son linealmente independientes).Ejercicio: Supongamos que tenemos los siguientes datos: precio de vivienda según metros, habitaciones y distancia al centro.Ajusta precio ~ metros (simple) y precio ~ metros + habitaciones + distancia_centro (múltiple).Compara \\(R^2\\), \\(R^2\\) ajustado y σ (RMSE).Interpreta el coeficiente de distancia_centro.Revisa QQ-plot y residuales vs ajustados. ¿Algún patrón?","code":"\n# Forma general\najuste <- lm(y ~ x1 + x2 + ... + xp, data = datos)\n# summary(ajuste)\ncomp <- dplyr::bind_rows(\n  glance(m1) %>% mutate(modelo=\"simple\"),\n  glance(m2) %>% mutate(modelo=\"multiple\")\n) %>% select(modelo, r.squared, adj.r.squared)\ncomp## # A tibble: 2 × 3\n##   modelo   r.squared adj.r.squared\n##   <chr>        <dbl>         <dbl>\n## 1 simple       0.824         0.802\n## 2 multiple     0.895         0.843\n# Modelo m2\npar(mfrow=c(1,2))\nplot(m2, which=1)  # Residuales vs ajustados\nplot(m2, which=2)  # QQ-plot\nset.seed(42)\nn <- 14\ncasas <- tibble::tibble(\n  precio = c(200,220,250,275,300,180,210,260,280,320,190,240,230,305),\n  metros = c(80,90,100,110,120,70,85,105,115,130,75,95,92,125),\n  habitaciones = c(2,3,3,4,4,2,3,3,4,5,2,3,3,4),\n  distancia_centro = c(5,4,6,3,2,8,6,3,2,1,7,5,4,2)\n)\ncasas## # A tibble: 14 × 4\n##    precio metros habitaciones distancia_centro\n##     <dbl>  <dbl>        <dbl>            <dbl>\n##  1    200     80            2                5\n##  2    220     90            3                4\n##  3    250    100            3                6\n##  4    275    110            4                3\n##  5    300    120            4                2\n##  6    180     70            2                8\n##  7    210     85            3                6\n##  8    260    105            3                3\n##  9    280    115            4                2\n## 10    320    130            5                1\n## 11    190     75            2                7\n## 12    240     95            3                5\n## 13    230     92            3                4\n## 14    305    125            4                2\nm_s <- lm(precio ~ metros, data=casas)\nm_m <- lm(precio ~ metros + habitaciones + distancia_centro, data=casas)\n\nbroom::glance(m_s)[,c(\"r.squared\",\"adj.r.squared\")]## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.996         0.996\nbroom::glance(m_m)[,c(\"r.squared\",\"adj.r.squared\")]## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.997         0.996\nbroom::tidy(m_m)## # A tibble: 4 × 5\n##   term             estimate std.error statistic      p.value\n##   <chr>               <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept)        -8.67     14.9      -0.583 0.573       \n## 2 metros              2.53      0.162    15.6   0.0000000236\n## 3 habitaciones       -0.505     2.80     -0.180 0.861       \n## 4 distancia_centro    1.38      0.974     1.42  0.187\npar(mfrow=c(1,2))\nplot(m_m, which=1)\nplot(m_m, which=2)"},{"path":"análisis-de-componentes-principales.html","id":"análisis-de-componentes-principales","chapter":"Sección 3 Análisis de Componentes Principales","heading":"Sección 3 Análisis de Componentes Principales","text":"","code":""},{"path":"análisis-factorial.html","id":"análisis-factorial","chapter":"Sección 4 Análisis Factorial","heading":"Sección 4 Análisis Factorial","text":"","code":""},{"path":"análisis-de-conglomerados.html","id":"análisis-de-conglomerados","chapter":"Sección 5 Análisis de Conglomerados","heading":"Sección 5 Análisis de Conglomerados","text":"","code":""},{"path":"análisis-de-discriminante.html","id":"análisis-de-discriminante","chapter":"Sección 6 Análisis de Discriminante","heading":"Sección 6 Análisis de Discriminante","text":"","code":""},{"path":"apéndices.html","id":"apéndices","chapter":"Sección 7 Apéndices","heading":"Sección 7 Apéndices","text":"","code":""},{"path":"apéndices.html","id":"introducción-a-r","chapter":"Sección 7 Apéndices","heading":"7.1 Introducción a R","text":"","code":""},{"path":"apéndices.html","id":"git-github","chapter":"Sección 7 Apéndices","heading":"7.2 Git + Github","text":"","code":""},{"path":"apéndices.html","id":"gráficas-multivariadas","chapter":"Sección 7 Apéndices","heading":"7.3 Gráficas Multivariadas","text":"","code":""},{"path":"apéndices.html","id":"escalas-de-medición","chapter":"Sección 7 Apéndices","heading":"7.4 Escalas de Medición","text":"","code":""},{"path":"apéndices.html","id":"valores-faltantes","chapter":"Sección 7 Apéndices","heading":"7.5 Valores Faltantes","text":"","code":""}]
