[{"path":"index.html","id":"estadística-multivariada","chapter":"Sección 1 Estadística Multivariada","heading":"Sección 1 Estadística Multivariada","text":"","code":""},{"path":"index.html","id":"temario","chapter":"Sección 1 Estadística Multivariada","heading":"1.1 Temario","text":"Regresión múltiple1.1 Mínimos cuadrados.1.2 Medidas de bondad de ajuste.1.3 Determinación del número de variables predictorias.Análisis de componentes principales2.1 Descripción de la metodología.2.2 Técnicas de extracción de componentes principales.2.3 Determinación del número de componentes principales.Análisis factorial3.1 Descripción de la metodología del análisis factorial.3.2 Descripción del modelo básico.3.3 Método de cálculo.3.4 Comparación con la técnica del análisis de componentes principales.3.5 Usos de software (R, Minitab, SciPy, entre otros).Análisis de conglomerados4.1Descripción de la metodología de análisis de conglomerados.4.2 Técnicas de jerarquización y de particionamiento.4.3 Implementación computacional.4.4 Usos de los dendogramas.4.5 Usos de software (R, Minitab, SciPy, entre otros).Análisis discriminante5.1 Descripción de la metodología del análisis discriminante.5.2 Discriminación entre dos grupos.5.3 Contribución por variable.5.4 Discriminación logística.5.5 Discriminación múltiple.5.6 Usos de software (R, Minitab, SciPy, entre otros).A1. RA2. Git + GithubA3. Gráficas MultivariadasA4. Escalas de MediciónA5. Valores Faltantes","code":""},{"path":"index.html","id":"evaluación","chapter":"Sección 1 Estadística Multivariada","heading":"1.2 Evaluación","text":"Examenes 50%Tareas 25%Proyecto 20%DataCamp 5%","code":""},{"path":"index.html","id":"proyecto-final","chapter":"Sección 1 Estadística Multivariada","heading":"1.3 Proyecto final","text":"Buscar una base de datos “real”Aplicar 3 métodos de estadística multivariadaEntregar documento con:\nDescripción de los datos\nPlanteamiento del problema\nMétodos usados\nInterpretación de resultados\nCódigo usado\nDescripción de los datosPlanteamiento del problemaMétodos usadosInterpretación de resultadosCódigo usadoRepositorio con código reproducibleExposición de resultados","code":""},{"path":"index.html","id":"referencias","chapter":"Sección 1 Estadística Multivariada","heading":"1.4 Referencias","text":"[1]","code":""},{"path":"index.html","id":"material-interesante","chapter":"Sección 1 Estadística Multivariada","heading":"1.5 Material interesante","text":"Bookdown.Software Carpentry.GitWhy GitR Markdown CookbookSTHDAYaRrr! Pirate’s Guide RLearn ggplot2 Using Shiny AppGgplot2: Elegant Graphics Data Analysis\nVersión online\nVersión onlineUse R! Colección SpringerLattice: Multivariate Data Visualization RR Graphics cookbookCuenta pro de Github","code":""},{"path":"index.html","id":"datacamp","chapter":"Sección 1 Estadística Multivariada","heading":"1.6 DataCamp","text":"","code":""},{"path":"regresión-múltiple.html","id":"regresión-múltiple","chapter":"Sección 2 Regresión múltiple","heading":"Sección 2 Regresión múltiple","text":"","code":""},{"path":"regresión-múltiple.html","id":"por-qué-estadística-multivariada","chapter":"Sección 2 Regresión múltiple","heading":"2.1 ¿Por qué estadística multivariada?","text":"El proceso de modelado consiste en construir expresiones matemáticas que permitan representar el comportamiento de una variable que queremos estudiar. Cuando contamos con varias variables, suele interesarnos analizar cómo unas influyen sobre otras, determinando si existe una relación, su intensidad y su forma. En muchos casos, estas relaciones pueden ser complejas y difíciles de describir directamente; por ello, se busca aproximarlas mediante funciones matemáticas sencillas como polinomios, que conserven los elementos esenciales para explicar el fenómeno de interés.Cuando estudiamos fenómenos deterministas, es común vincular una variable dependiente con una o más variables independientes. Por ejemplo, en la ecuación de la velocidad (\\(v=d/t\\)), la distancia depende de la velocidad y del tiempo. En la práctica, cuando realizamos distintos experimentos, las fórmulas deterministas podrían capturar por completo el comportamiento observado. Esto puede deberse factores controlados, la presencia de variabilidad natural o efectos aleatorios. Por esta razón, además de la parte determinista del modelo, se incorpora un término que represente la discrepancia aleatoria entre lo que se predice y lo que efectivamente se observa. De forma general, esta idea se resume como:\\[Observación = Modelo \\ + \\ Error\\]Cuando se supone que la relación entre las variables puede representarse mediante una ecuación lineal, hablamos de análisis de regresión lineal. Si intervienen únicamente dos variables, una dependiente \\(y\\) y independiente \\(x\\), se trata de regresión lineal simple. En cambio, cuando la variable de interés \\(y\\) depende de dos o más variables independientes \\(x_1,x_2, ...\\) hablamos de regresión lineal múltiple.Supongamos que queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia?En este caso se tiene que el puntaje o rendimiento lo podemos representar con \\(y\\) y las horas de estudio con \\(x\\). Entonces esta propuesta de modelo, la podríamos representar como:\\[y=\\beta_0+\\beta_1x\\]\nDonde \\(\\beta_0\\) es la ordenada al origen y \\(\\beta_1\\) la pendiente. Esta recta podría ajustarse al modelo por diferentes razones, entonces lo que se hace es considerar un error aleatorio \\(\\epsilon\\). El modelo que ya considera este error se representa como:\\[y=\\beta_0+\\beta_1x+\\epsilon.\\]este modelo se le conoce como modelo de regresión lineal simple y \\(\\beta_0,\\beta_1\\) se les conoce como coeficientes de regresión.En problemas reales, casi nunca una sola variable explica el fenómeno. Las decisiones y predicciones mejoran cuando integramos múltiples fuentes de información.Ejemplos:\n- Salud: riesgo de una enfermedad según edad, IMC, actividad física, dieta y antecedentes.\n- Ingeniería: vida útil de una pieza según temperatura, vibración, material y carga.\n- Biología: crecimiento de una planta por agua, luz, fertilizante, temperatura.Ejemplo: Si queremos predecir el rendimiento académico de un estudiante, ¿solo necesitamos las horas que estudia? ¿qué otras variables podrían influir en el puntaje de un examen?¿Qué pasa si solo graficamos horas de estudio vs puntaje?¿Se ajusta un modelo lineal? ¿Porqué?","code":"\nset.seed(123)\nn <- 10\ndata_intro <- tibble(\n  estudiante = paste0(\"E\", 1:n),\n  horas_estudio = c(2,3,4,5,1,3,2,4,5,6),\n  horas_sueno  = c(7,8,6,7,5,8,7,6,9,7),\n  asistencia   = c(0.9,0.95,0.8,0.85,0.7,0.9,0.8,0.9,1,0.95),\n  puntaje      = c(65,70,68,80,60,75,65,78,88,85)\n)\ndata_intro## # A tibble: 10 × 5\n##    estudiante horas_estudio horas_sueno asistencia puntaje\n##    <chr>              <dbl>       <dbl>      <dbl>   <dbl>\n##  1 E1                     2           7       0.9       65\n##  2 E2                     3           8       0.95      70\n##  3 E3                     4           6       0.8       68\n##  4 E4                     5           7       0.85      80\n##  5 E5                     1           5       0.7       60\n##  6 E6                     3           8       0.9       75\n##  7 E7                     2           7       0.8       65\n##  8 E8                     4           6       0.9       78\n##  9 E9                     5           9       1         88\n## 10 E10                    6           7       0.95      85\nlibrary(ggplot2)\nggplot(data_intro, aes(horas_estudio, puntaje)) +\n  geom_point(size=3) +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(title=\"¿Solo horas de estudio explican el puntaje?\")## `geom_smooth()` using formula = 'y ~ x'"},{"path":"regresión-múltiple.html","id":"qué-es-multivariado-y-por-qué-lo-necesitamos","chapter":"Sección 2 Regresión múltiple","heading":"2.1.1 ¿Qué es “multivariado” y por qué lo necesitamos?","text":"Idea central: cuando varias \\(x\\) influyen sobre \\(y\\), estudiar cada \\(x\\) por separado puede engañarnos. El análisis multivariado permite:Aislar efectos: estimar el efecto de \\(x_1\\) manteniendo constantes \\(x_2,x_3,...\\).Mejorar predicción: reducir error al añadir información relevante.Controlar confusores: variables que cambian la relación aparente entre \\(y\\) y \\(x\\).Ejemplo: Si ajustamos ahora un modelo con varias variables, ¿vamos observar un cambio? ¿se ajustará mejor?¿Aumentó \\(R^2\\) al incluir más variables? ¿Por qué tiende subir?¿Qué cambia en la interpretación de horas_estudio al controlar por horas_sueno y asistencia?¿Puede un predictor ser importante en bivariado y en multivariado (o viceversa)?","code":"\n# Modelo simple\nm1 <- lm(puntaje ~ horas_estudio, data = data_intro)\n\n# Modelo múltiple\nm2 <- lm(puntaje ~ horas_estudio + horas_sueno + asistencia, data = data_intro)\n\n# Medidas clave\nR2_m1  <- glance(m1)$r.squared\nR2_m2  <- glance(m2)$r.squared\n\nprint(paste(\"El R2 del modelo simple:\", R2_m1))## [1] \"El R2 del modelo simple: 0.824317362184441\"\nprint(paste(\"El R2 del modelo multiple:\", R2_m2))## [1] \"El R2 del modelo multiple: 0.895428180549875\"\n#R2adj_m1 <- glance(m1)$adj.r.squared\n#R2adj _m2 <- glance(m2)$adj.r.squared"},{"path":"regresión-múltiple.html","id":"regresión-múltiple-1","chapter":"Sección 2 Regresión múltiple","heading":"2.2 Regresión múltiple","text":"","code":""},{"path":"regresión-múltiple.html","id":"modelo-y-estimación","chapter":"Sección 2 Regresión múltiple","heading":"2.2.1 Modelo y estimación","text":"Los modelos en regresión lineal múltiple están dados por la siguiente forma, donde \\(y\\) depende de \\(p\\) variables predictoras:\\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2} + \\beta_px_{ip}+\\epsilon_i.\\]Se suele asumir que los errores \\(\\epsilon_i\\) son ..d. con distribución normal de media 0 y varianza \\(\\sigma^2\\) desconocida. Los coeficientes \\(\\beta_i\\) son constantes desconocidas y son los parámetros del modelo. Cada \\(\\beta_j\\) representa el cambio esperado en la respuesta \\(y\\) por el cambio unitario en \\(x_i\\) cuando todas las demás variables independientes \\(x_i(\\neq j)\\) se mantienen constantes.Los coeficientes los podemos interpretar como sigue:Intercepto (\\(\\beta_0\\)): valor esperado de \\(y\\) cuando todas las \\(x\\)=0.Pendiente \\(\\beta_j\\): efecto parcial de \\(x_j\\) sobre \\(y\\) manteniendo las demás constantes.En los modelos de regreción lineal, solemos usar las siguientes medidas de bondad de ajuste:\\(R^2\\): proporción de varianza de \\(y\\) explicada.\\(R^2\\) ajustado: penaliza por número de predictores (mejor para comparar modelos con distinto número de x).RMSE (\\(\\sigma\\)): error típico de predicción en unidades de \\(y\\).Para este modelo algunos de los supuestos se siguen del modelo de regresión lineal\nsimple y se agregan algunos que tienen que ver con la relación que pudiera existir entre\nlas variables regresoras.El modelo es lineal en los parámetros.Chequeo: residuales vs ajustados sin patrón claro.El modelo está especificado correctamente.Covarianza cero entre variables regresoras y el error.Esperanza del error igual cero.Homocedasticidad.autocorrelación entre los errores.Los errores siguen una distribución normal.Mas observaciones que parámetros estimar.Variación entre los valores de las variables regresoras.colinealidad (multicolinealidad) entre las variables regresoras, es decir, existe\nuna relación lineal entre \\(x_i\\) y \\(x_j\\) (es decir, las variables son linealmente independientes).Ejercicio: Supongamos que tenemos los siguientes datos: precio de vivienda según metros, habitaciones y distancia al centro.Ajusta precio ~ metros (simple) y precio ~ metros + habitaciones + distancia_centro (múltiple).Compara \\(R^2\\), \\(R^2\\) ajustado y σ (RMSE).Interpreta el coeficiente de distancia_centro.Revisa QQ-plot y residuales vs ajustados. ¿Algún patrón?","code":"\n# Forma general\najuste <- lm(y ~ x1 + x2 + ... + xp, data = datos)\n# summary(ajuste)\ncomp <- dplyr::bind_rows(\n  glance(m1) %>% mutate(modelo=\"simple\"),\n  glance(m2) %>% mutate(modelo=\"multiple\")\n) %>% select(modelo, r.squared, adj.r.squared)\ncomp## # A tibble: 2 × 3\n##   modelo   r.squared adj.r.squared\n##   <chr>        <dbl>         <dbl>\n## 1 simple       0.824         0.802\n## 2 multiple     0.895         0.843\n# Modelo m2\npar(mfrow=c(1,2))\nplot(m2, which=1)  # Residuales vs ajustados\nplot(m2, which=2)  # QQ-plot\nset.seed(42)\nn <- 14\ncasas <- tibble::tibble(\n  precio = c(200,220,250,275,300,180,210,260,280,320,190,240,230,305),\n  metros = c(80,90,100,110,120,70,85,105,115,130,75,95,92,125),\n  habitaciones = c(2,3,3,4,4,2,3,3,4,5,2,3,3,4),\n  distancia_centro = c(5,4,6,3,2,8,6,3,2,1,7,5,4,2)\n)\ncasas## # A tibble: 14 × 4\n##    precio metros habitaciones distancia_centro\n##     <dbl>  <dbl>        <dbl>            <dbl>\n##  1    200     80            2                5\n##  2    220     90            3                4\n##  3    250    100            3                6\n##  4    275    110            4                3\n##  5    300    120            4                2\n##  6    180     70            2                8\n##  7    210     85            3                6\n##  8    260    105            3                3\n##  9    280    115            4                2\n## 10    320    130            5                1\n## 11    190     75            2                7\n## 12    240     95            3                5\n## 13    230     92            3                4\n## 14    305    125            4                2\nm_s <- lm(precio ~ metros, data=casas)\nm_m <- lm(precio ~ metros + habitaciones + distancia_centro, data=casas)\n\nbroom::glance(m_s)[,c(\"r.squared\",\"adj.r.squared\")]## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.996         0.996\nbroom::glance(m_m)[,c(\"r.squared\",\"adj.r.squared\")]## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.997         0.996\nbroom::tidy(m_m)## # A tibble: 4 × 5\n##   term             estimate std.error statistic      p.value\n##   <chr>               <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept)        -8.67     14.9      -0.583 0.573       \n## 2 metros              2.53      0.162    15.6   0.0000000236\n## 3 habitaciones       -0.505     2.80     -0.180 0.861       \n## 4 distancia_centro    1.38      0.974     1.42  0.187\npar(mfrow=c(1,2))\nplot(m_m, which=1)\nplot(m_m, which=2)"},{"path":"regresión-múltiple.html","id":"estimación-de-parámetros","chapter":"Sección 2 Regresión múltiple","heading":"2.3 Estimación de parámetros","text":"Ejemplo (Montgomery, 2002): : Un embotellador de bebidas gaseosas analiza las rutas de servicio de las máquinas expendedoras en su sistema de distribución. Le interesa predecir el tiempo necesario para que el representante de ruta atienda las máquinas expendedoras en una tienda.Esta actividad de servicio consiste en abastecer la máquina con productos embotellados, y algo de mantenimiento o limpieza. El ingeniero industrial responsable del estudio ha sugerido que las dos variables más importantes que afectan el tiempo de entrega \\(y\\) son la cantidad de cajas de producto abastecido, \\(x_1\\), y la distancia caminada por el representante, \\(x_2\\).El ingeniero ha reunido 25 observaciones de tiempo de entrega que se ven en la tabla siguiente. Se ajustará el modelo de regresión lineal multiple siguiente:\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +  \\varepsilon\n\\]Archivo: refrescos.csv.Veamos un gráfico de dispersión de los datos. ¿Qué observamos?Estimar \\(\\beta\\)Primero, vamos crear la matriz \\(X\\) y el vector \\(y\\).Ya sabemos que nuestro estimador está dado por\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]Entonces podemos encontrar el estimador.Entonces el ajuste por el método de mínimos cuadrados, con los coeficientes de regresión que encontramos está dado por:\\(\\hat{y} =\\) 2.3412311 \\(+\\) 1.6159072 \\(x_1\\) \\(+\\) 1.6159072 \\(x_2\\)Esto lo podemos hacer más rápido usando la función de lm. Construimos el modelo.¿Cómo accedemos los valores del modelo?Los valores son \\(\\beta_0=\\) 2.3412311, \\(\\beta_1=\\) 1.6159072 y \\(\\beta_2=\\) 0.0143848.Estimación de la varianza del error \\(\\sigma^2\\)Ya tenemos que la suma de los cuadrados de los errores está dada por\\[ SSE = y'y - \\hat{\\beta}X'y\\]Sustituimos los valores que tenemos y obtemos el SSE.Y de está forma, podemos encontrar el estimador de \\(\\sigma^2\\).Directo con las funciones de R, podemos acceder los parámetros que se guardaron en el modelo que ya calculamos.Algunos de los parámetros almacenados en el modelo nos permiten obtener también el resultado previo.","code":"\n# \ndatos <- data.frame(\n  Observacion = 1:25,\n  y = c(16.68, 11.50, 12.03, 14.88, 13.75,\n        18.11, 8.00, 17.83, 79.24, 21.50,\n        40.33, 21.00, 13.50, 19.75, 24.00,\n        29.00, 15.35, 19.00, 9.50, 35.10,\n        17.90, 52.32, 18.75, 19.83, 10.75),\n  x1 = c(7, 3, 3, 4, 6,\n         7, 2, 7, 30, 5,\n         16, 10, 4, 6, 9,\n         10, 6, 7, 3, 17,\n         10, 26, 9, 8, 4),\n  x2 = c(560, 220, 340, 80, 150,\n         330, 110, 210, 1460, 605,\n         688, 215, 255, 462, 448,\n         776, 200, 132, 36, 770,\n         140, 810, 450, 635, 150)\n)\n\ndatos##    Observacion     y x1   x2\n## 1            1 16.68  7  560\n## 2            2 11.50  3  220\n## 3            3 12.03  3  340\n## 4            4 14.88  4   80\n## 5            5 13.75  6  150\n## 6            6 18.11  7  330\n## 7            7  8.00  2  110\n## 8            8 17.83  7  210\n## 9            9 79.24 30 1460\n## 10          10 21.50  5  605\n## 11          11 40.33 16  688\n## 12          12 21.00 10  215\n## 13          13 13.50  4  255\n## 14          14 19.75  6  462\n## 15          15 24.00  9  448\n## 16          16 29.00 10  776\n## 17          17 15.35  6  200\n## 18          18 19.00  7  132\n## 19          19  9.50  3   36\n## 20          20 35.10 17  770\n## 21          21 17.90 10  140\n## 22          22 52.32 26  810\n## 23          23 18.75  9  450\n## 24          24 19.83  8  635\n## 25          25 10.75  4  150\npairs(datos[-1])\n# Columna de 1 para el intercepto\nidv <- rep(1, nrow(datos))\n# Creamos matriz X\nX <- matrix(c(idv,datos$x1,datos$x2),nrow=25,ncol=3)\n# Creamos el vector y\ny <- matrix(datos$y, nrow = 25, ncol = 1)\nbeta <- solve(t(X) %*% X) %*% t(X) %*% y\nbeta##            [,1]\n## [1,] 2.34123115\n## [2,] 1.61590721\n## [3,] 0.01438483\nM1 <- lm(y ~ x1 + x2, datos)\nM1 ## \n## Call:\n## lm(formula = y ~ x1 + x2, data = datos)\n## \n## Coefficients:\n## (Intercept)           x1           x2  \n##     2.34123      1.61591      0.01438\nbeta_0 <- M1$coefficients[1]\nbeta_1 <- M1$coefficients[2]\nbeta_2 <- M1$coefficients[3]\nSSE <- t(y)%*% y - t(beta) %*% t(X) %*% y\nSSE##          [,1]\n## [1,] 233.7317\nvarest <- SSE / (nrow(y) - nrow(beta))\nvarest##          [,1]\n## [1,] 10.62417\nsummary(M1)## \n## Call:\n## lm(formula = y ~ x1 + x2, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7880 -0.6629  0.4364  1.1566  7.4197 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 2.341231   1.096730   2.135 0.044170 *  \n## x1          1.615907   0.170735   9.464 3.25e-09 ***\n## x2          0.014385   0.003613   3.981 0.000631 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.259 on 22 degrees of freedom\n## Multiple R-squared:  0.9596, Adjusted R-squared:  0.9559 \n## F-statistic: 261.2 on 2 and 22 DF,  p-value: 4.687e-16\nsum(residuals(M1)^2) / df.residual(M1)## [1] 10.62417"},{"path":"regresión-múltiple.html","id":"ejercicios","chapter":"Sección 2 Regresión múltiple","heading":"2.3.1 Ejercicios","text":"Ejercicio 1: Un analista hace un estudio químico y espera que el rendimiento de cierta sustancia se vea afectado por dos factores. Se realizan 17 experimentos cuyos datos se registran en el cuadro siguiente. Por experimentos similares, se sabe que los factores \\(x_1\\) y \\(x_2\\) están relacionados; por ello, el analista decide utilizar un modelo de regresión lineal múltiple. Calcule el modelo de regresión y grafíquelo sobre las observaciones.Archivo: est_quimico.csvEjercicio 2: Repetir el ejemplo con los datos datasets::trees de R que proporciona mediciones\ndel diámetro, altura y volumen de madera en 31 cerezos negros talados.Ejercicio 3: Subir Github los dos ejercicios previos tanto con solución en R como en Python. Comparar las funciones. Ventajas y desventajas de ambas.","code":"\ndatos2 <- data.frame(\n  Experimento = 1:17,\n  x1 = c(41.9, 43.4, 43.9, 44.5, 47.3, 47.5, 47.9, 50.2, 52.8, 53.2, 56.7, 57.0, 63.5, 64.3, 71.1, 77.0, 77.8),\n  x2 = c(29.1, 29.3, 29.5, 29.7, 29.9, 30.3, 30.5, 30.7, 30.8, 30.9, 31.5, 31.7, 31.9, 32.0, 32.1, 32.5, 32.9),\n  y  = c(251.3, 251.3, 248.3, 267.5, 273.0, 276.5, 270.3, 274.9, 285.0, 290.0, 297.0, 302.5, 304.5, 309.3, 321.7, 330.7, 349.0)\n)\n\ndatos2##    Experimento   x1   x2     y\n## 1            1 41.9 29.1 251.3\n## 2            2 43.4 29.3 251.3\n## 3            3 43.9 29.5 248.3\n## 4            4 44.5 29.7 267.5\n## 5            5 47.3 29.9 273.0\n## 6            6 47.5 30.3 276.5\n## 7            7 47.9 30.5 270.3\n## 8            8 50.2 30.7 274.9\n## 9            9 52.8 30.8 285.0\n## 10          10 53.2 30.9 290.0\n## 11          11 56.7 31.5 297.0\n## 12          12 57.0 31.7 302.5\n## 13          13 63.5 31.9 304.5\n## 14          14 64.3 32.0 309.3\n## 15          15 71.1 32.1 321.7\n## 16          16 77.0 32.5 330.7\n## 17          17 77.8 32.9 349.0"},{"path":"regresión-múltiple.html","id":"pruebas-de-hipótesis","chapter":"Sección 2 Regresión múltiple","heading":"2.4 Pruebas de Hipótesis","text":"Cuando revisamos el summary del modelo, nos arroja si son significativas o y que nivel de significancia las variables que estamos considerando. Veamos el siguiente ejemplo.","code":""},{"path":"regresión-múltiple.html","id":"prueba-de-la-significancia-de-la-regresión","chapter":"Sección 2 Regresión múltiple","heading":"2.4.1 Prueba de la significancia de la regresión","text":"Ejemplo: Con los datos del embotellador de bebidas gaseosas, se probará la significancia de la regresión.Para probar\\[H_0 : \\beta_1 = \\beta_2=0\\]se calcula el estadístico:Como el valor de \\(F_0\\) es mayor que el valor tabulado de \\(F_{\\alpha;p,n-p-1}=F_{0.05;2;22}=3.44\\), se rechaza \\(H_0\\). Lo cual implica qye el tiempo de entrega depende del volumen de entrega y/o de la distancia.Ahora, usando los modelos que ya calculamos.","code":"\nSCT <- t(y) %*% y -  sum(y)**2 / nrow(datos)\nSCT##          [,1]\n## [1,] 5784.543\nSCE <- t(beta) %*% t(X) %*% y - sum(y)**2 / nrow(datos)\nSCE##          [,1]\n## [1,] 5550.811\nSSE <- SCT - SCE\nSSE##          [,1]\n## [1,] 233.7317\nF0 <- (SCE / (ncol(X) - 1)) / (SSE / (nrow(X) - (ncol(X) -1) - 1))\nF0##          [,1]\n## [1,] 261.2351\nSCT.m<-sum((datos$y-mean(datos$y))^2)\nSCT.m## [1] 5784.543\nSCE.m <-sum((M1$fitted-mean(datos$y))^2)\nSCE.m## [1] 5550.811\nSSE.m <-sum(M1$residuals^2)\nSSE.m## [1] 233.7317\nn<-nrow(y)\nn## [1] 25\nGLT<- n-1\nGLT## [1] 24\nGLRes<- df.residual(M1)\nGLRes## [1] 22\nGLR<- GLT-GLRes\nGLR## [1] 2\nCMR <- SCE /GLR\nCMR##          [,1]\n## [1,] 2775.405\nCMRes <- SSE / GLRes\nCMRes##          [,1]\n## [1,] 10.62417\nF0 <- CMR/CMRes\nF0##          [,1]\n## [1,] 261.2351\npv <- 1 - pf(F0, GLR,GLRes)\npv##              [,1]\n## [1,] 4.440892e-16\nalpha <- 0.05; df1 <- 2; df2 <- 22\nF_crit <- qf(1 - alpha, df1, df2)\nF_crit## [1] 3.443357"},{"path":"regresión-múltiple.html","id":"pruebas-sobre-coeficientes-individuales-de-regresión","chapter":"Sección 2 Regresión múltiple","heading":"2.4.2 Pruebas sobre coeficientes individuales de regresión","text":"Ejemplo: Usando los datos del embotellador de bebidas gaseosas, se desea evaluar la importancia de la variable regresora distancia (\\(x_2\\)) dado que el regresor cajas (\\(x_1\\)) está en el modelo.Usando el modelo que ya tenemos calculado M1 podemos obtener estos mismos resultados de la siguiente forma.","code":"\nC22 <- solve(t(X) %*% X)[3,3]\nC22## [1] 1.228745e-06\nt0 <- beta_2 / sqrt(varest * C22)\nt0##          [,1]\n## [1,] 3.981313\n## t tabulado con confianza 95% y 22 grados de libertad\ntt <- qt(p = 0.95 + 0.05/2, df = 22, lower.tail = TRUE)\ntt## [1] 2.073873\nsummary(M1)## \n## Call:\n## lm(formula = y ~ x1 + x2, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7880 -0.6629  0.4364  1.1566  7.4197 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 2.341231   1.096730   2.135 0.044170 *  \n## x1          1.615907   0.170735   9.464 3.25e-09 ***\n## x2          0.014385   0.003613   3.981 0.000631 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.259 on 22 degrees of freedom\n## Multiple R-squared:  0.9596, Adjusted R-squared:  0.9559 \n## F-statistic: 261.2 on 2 and 22 DF,  p-value: 4.687e-16"},{"path":"regresión-múltiple.html","id":"intervalos-de-confianza","chapter":"Sección 2 Regresión múltiple","heading":"2.5 Intervalos de confianza","text":"","code":""},{"path":"regresión-múltiple.html","id":"intervalos-de-confianza-en-los-coeficientes-de-regresión","chapter":"Sección 2 Regresión múltiple","heading":"2.5.1 Intervalos de confianza en los coeficientes de regresión","text":"Ejemplo: Usando los datos del embotellador de bebidas gaseosas, queremos calcular el intervalo de confianza del 95% para \\(\\beta_1\\). Recordemos que el estimador puntual de \\(\\beta_1\\) es 1.6159072.","code":"\nC11 <- solve(t(X) %*% X)[2,2]\n\nizq <- beta_1 - tt * sqrt(varest*C11)\nizq##          [,1]\n## [1,] 1.261825\nder <- beta_1 + tt * sqrt(varest*C11)\nder##         [,1]\n## [1,] 1.96999"},{"path":"regresión-múltiple.html","id":"intervalo-de-confianza-de-la-respuesta-media","chapter":"Sección 2 Regresión múltiple","heading":"2.5.2 Intervalo de confianza de la respuesta media","text":"Ejemplo: El embotellador de bebidas gaseosas quiere establecer un intervalo de confianza del 95% para el tiempo medio de entrega para una tienda donde se requieren \\(x_1=8\\) cajas y la distancia es de \\(x_2=275\\) pies.El valor ajustado en ese punto es:La varianza de \\(\\hat{y_0}\\)Entonces el intervalo de confianza en este punto es:Ejemplo:  Usaremos el conjunto de datos data(\"marketing\") que contiene 200 observaciones de un experimento publicitario que evalúa el impacto de tres medios de anuncio en las ventas. Para cada observación se registran los presupuestos de publicidad (en miles de dólares) y las ventas obtenidas.\nVariables:youtube: presupuesto invertido en anuncios de YouTube (miles de USD).facebook: presupuesto invertido en Facebook (miles de USD).newspaper: presupuesto invertido en prensa escrita (miles de USD).sales: ventas registradas (variable respuesta).Cargamos los datos:Exploramos rápidamente la base para ver qué variables contiene y la dimensión:Ajustamos un modelo lineal que incluya todas las variables, es decir,\\(sales=\\beta_0+\\beta_1 youtube+\\beta_2 facebook+ \\beta_3 newspaper+\\epsilon\\)¿Qué se puede decir sobre la significancia de la variable \\(newspaper\\)?Veamos qué ocurre con el modelo al eliminar la variable \\(newspaper\\)Lo que sigue, es hacer pruebas de hipótesis tanto en las variables como en los coeficientes de regresión.Ejercicio 1: Realizar las pruebas de hipótesis sobre la significancia de la regresión y sobre los coeficientes. Encontrar los intervalos de confianza respectivos del 95%.\nPara una tienda con presupuestos:\n\\(youtube = 150\\), \\(facebook = 30\\), \\(newspaper=20\\) (en miles de USD):\n() Calcula el intervalo de confianza del 95% para la media de ventas \\(\\mathbb{E}(sales | X_0)\\).\n(b) Calcula el intervalo de predicción del 95% para una nueva observación de ventas.\n(c) Comenta la diferencia entre ambos intervalos.\nSubir respuesta y explicación de sus resultados github.","code":"\nX0 <- matrix(c(1, 8, 275), nrow = 3)\nX0##      [,1]\n## [1,]    1\n## [2,]    8\n## [3,]  275\ny0 <- t(X0) %*% beta\ny0##          [,1]\n## [1,] 19.22432\nvar_y0 <- varest * t(X0) %*% solve(t(X) %*% X) %*% X0\nvar_y0##           [,1]\n## [1,] 0.5734134\nl_izq <- y0 - tt * sqrt(var_y0) \nl_izq##         [,1]\n## [1,] 17.6539\nl_der <- y0 + tt * sqrt(var_y0) \nl_der##          [,1]\n## [1,] 20.79474\nlibrary(datarium)\ndata(\"marketing\")\nstr(marketing)## 'data.frame':    200 obs. of  4 variables:\n##  $ youtube  : num  276.1 53.4 20.6 181.8 217 ...\n##  $ facebook : num  45.4 47.2 55.1 49.6 13 ...\n##  $ newspaper: num  83 54.1 83.2 70.2 70.1 ...\n##  $ sales    : num  26.5 12.5 11.2 22.2 15.5 ...\n#?marketing\nmodelo1<-lm(sales~youtube+facebook+newspaper,data=marketing)\nsummary(modelo1)## \n## Call:\n## lm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.5932  -1.0690   0.2902   1.4272   3.3951 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.526667   0.374290   9.422   <2e-16 ***\n## youtube      0.045765   0.001395  32.809   <2e-16 ***\n## facebook     0.188530   0.008611  21.893   <2e-16 ***\n## newspaper   -0.001037   0.005871  -0.177     0.86    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.023 on 196 degrees of freedom\n## Multiple R-squared:  0.8972, Adjusted R-squared:  0.8956 \n## F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\nmodelo2<-lm(sales~facebook+youtube,data=marketing)\nsummary(modelo2)## \n## Call:\n## lm(formula = sales ~ facebook + youtube, data = marketing)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.5572  -1.0502   0.2906   1.4049   3.3994 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.50532    0.35339   9.919   <2e-16 ***\n## facebook     0.18799    0.00804  23.382   <2e-16 ***\n## youtube      0.04575    0.00139  32.909   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.018 on 197 degrees of freedom\n## Multiple R-squared:  0.8972, Adjusted R-squared:  0.8962 \n## F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16"},{"path":"regresión-múltiple.html","id":"ejercicios-regresión-lineal-multiple","chapter":"Sección 2 Regresión múltiple","heading":"2.6 Ejercicios Regresión Lineal Multiple","text":"Realiza los siguientes ejercicios. En cada inciso:explica y comenta la solución,explica y comenta la solución,incluye el código utilizado, yincluye el código utilizado, yañade las gráficas (plots) correspondientes con su interpretación.añade las gráficas (plots) correspondientes con su interpretación.Asegúrate de que el código sea reproducible y que las figuras tengan títulos, ejes y leyendas.Ejercicio 1: Para los datos de la Liga Nacional de Fútbol. Realizar tanto con las funciones de R y Python como con las fórmulas que usan matrices.Ajustar un modelo de regresión lineal múltiple que relacione la cantidad de juegos ganados con las yardas por aire del equipo (\\(x_2\\)), el porcentaje de jugadas por tierra (\\(x_7\\)) y las yardas por tierra del contrario (\\(x_8\\)).Ajustar un modelo de regresión lineal múltiple que relacione la cantidad de juegos ganados con las yardas por aire del equipo (\\(x_2\\)), el porcentaje de jugadas por tierra (\\(x_7\\)) y las yardas por tierra del contrario (\\(x_8\\)).Formar la tabla de análisis de varianza y probar la significancia de la regresión.Formar la tabla de análisis de varianza y probar la significancia de la regresión.Calcular el estadístico t para probar las hipótesis \\(H_0 : \\beta_2 = 0\\), \\(H_0 : \\beta_7 = 0\\) y \\(H_0 : \\beta_8 = 0\\). ¿Qué conclusiones se pueden sacar acerca del papel de las variables \\(x_2\\), \\(x_7\\) y \\(x_8\\) en el modelo?Calcular el estadístico t para probar las hipótesis \\(H_0 : \\beta_2 = 0\\), \\(H_0 : \\beta_7 = 0\\) y \\(H_0 : \\beta_8 = 0\\). ¿Qué conclusiones se pueden sacar acerca del papel de las variables \\(x_2\\), \\(x_7\\) y \\(x_8\\) en el modelo?Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo.Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo.Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?Calcular un intervalo de confianza de \\(95\\%\\) para \\(\\beta_7\\) y un intervalo de confianza de \\(95\\%\\) para la cantidad media de juegos ganados por un equipo cuando \\(x_2 = 2300\\), \\(x_7 = 56\\) y \\(x_8 = 2100\\).Calcular un intervalo de confianza de \\(95\\%\\) para \\(\\beta_7\\) y un intervalo de confianza de \\(95\\%\\) para la cantidad media de juegos ganados por un equipo cuando \\(x_2 = 2300\\), \\(x_7 = 56\\) y \\(x_8 = 2100\\).Ajustar un modelo esos datos, usando solo \\(x_7\\) y \\(x_8\\) como regresores y probar la significancia de la regresión.Ajustar un modelo esos datos, usando solo \\(x_7\\) y \\(x_8\\) como regresores y probar la significancia de la regresión.Calcular \\(R^2\\) y \\(R^2_{adj}\\). Compararlos con los resultados del modelo anterior.Calcular \\(R^2\\) y \\(R^2_{adj}\\). Compararlos con los resultados del modelo anterior.Calcular un intervalo de confianza de \\(95\\%\\) para \\(\\beta_7\\). También, un intervalo de confianza de \\(95\\%\\) para la cantidad media de juegos ganados por un equipo cuando \\(x_7 = 56\\) y \\(x_8 = 2100\\). Comparar las longitudes de esos intervalos de confianza con las longitudes de los correspondientes al modelo anterior.Calcular un intervalo de confianza de \\(95\\%\\) para \\(\\beta_7\\). También, un intervalo de confianza de \\(95\\%\\) para la cantidad media de juegos ganados por un equipo cuando \\(x_7 = 56\\) y \\(x_8 = 2100\\). Comparar las longitudes de esos intervalos de confianza con las longitudes de los correspondientes al modelo anterior.¿Qué conclusiones se pueden sacar de este problema, acerca de las consecuencias de omitir un regresor importante de un modelo?¿Qué conclusiones se pueden sacar de este problema, acerca de las consecuencias de omitir un regresor importante de un modelo?Ejericio 2: Véase los datos de rendimiento de gasolina. Realizar el ejercicio en R.Ajustar un modelo de regresión lineal múltiple que relacione el rendimiento de la gasolina y, en millas por galón, la cilindrada del motor (\\(x_1\\)) y la cantidad de gargantas del carburador (\\(x_6\\)).Ajustar un modelo de regresión lineal múltiple que relacione el rendimiento de la gasolina y, en millas por galón, la cilindrada del motor (\\(x_1\\)) y la cantidad de gargantas del carburador (\\(x_6\\)).Formar la tabla de análisis de varianza y probar la significancia de la regresión.Formar la tabla de análisis de varianza y probar la significancia de la regresión.Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo. Compararlas con las \\(R^2\\) y \\(R^2_{adj}\\) Ajustado para el modelo de regresión lineal simple, que relaciona las millas con la cilindrada.Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo. Compararlas con las \\(R^2\\) y \\(R^2_{adj}\\) Ajustado para el modelo de regresión lineal simple, que relaciona las millas con la cilindrada.Determinar un intervalo de confianza para \\(\\beta_1\\).Determinar un intervalo de confianza para \\(\\beta_1\\).Determinar un intervalo de confianza de \\(95\\%\\) para el rendimiento promedio de la gasolina, cuando \\(x_1 = 225 pulg^3\\) y \\(x_6 = 2\\) gargantas.Determinar un intervalo de confianza de \\(95\\%\\) para el rendimiento promedio de la gasolina, cuando \\(x_1 = 225 pulg^3\\) y \\(x_6 = 2\\) gargantas.Determinar un intervalo de predicción de \\(95\\%\\) para una nueva observación de rendimiento de gasolina, cuando \\(x_1 = 225 pulg^3\\) y \\(x_6 = 2\\) gargantas.Determinar un intervalo de predicción de \\(95\\%\\) para una nueva observación de rendimiento de gasolina, cuando \\(x_1 = 225 pulg^3\\) y \\(x_6 = 2\\) gargantas.Considerar el modelo de regresión lineal simple, que relaciona las millas con la cilindrada. Construir un intervalo de confianza de \\(95\\%\\) para el rendimiento promedio de la gasolina y un intervalo de predicción para el rendimiento, cuando \\(x_1 = 225 pulg^3\\). Comparar las longitudes de estos intervalos con los intervalos obtenidos en los dos incisos anteriores. ¿Tiene ventajas agregar \\(x_6\\) al modelo?Considerar el modelo de regresión lineal simple, que relaciona las millas con la cilindrada. Construir un intervalo de confianza de \\(95\\%\\) para el rendimiento promedio de la gasolina y un intervalo de predicción para el rendimiento, cuando \\(x_1 = 225 pulg^3\\). Comparar las longitudes de estos intervalos con los intervalos obtenidos en los dos incisos anteriores. ¿Tiene ventajas agregar \\(x_6\\) al modelo?Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?Ejercicio 3: Véase los datos sobre precios de viviendas. Realizar el ejercicio en Python.Ajustar un modelo de regresión lineal múltiple que relacione el precio de venta con los nueve regresores.Ajustar un modelo de regresión lineal múltiple que relacione el precio de venta con los nueve regresores.Probar la significancia de la regresión.¿Qué conclusiones se pueden sacar?Probar la significancia de la regresión.¿Qué conclusiones se pueden sacar?Usar pruebas t para evaluar la contribución de cada regresor al modelo.Usar pruebas t para evaluar la contribución de cada regresor al modelo.Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo.Calcular \\(R^2\\) y \\(R^2_{adj}\\) para este modelo.¿Cuál es la contribución del tamaño del lote y el espacio vital para el modelo, dado que se incluyeron todos los demás regresores?.¿Cuál es la contribución del tamaño del lote y el espacio vital para el modelo, dado que se incluyeron todos los demás regresores?.En este modelo, ¿la colinealidad es un problema potencial?En este modelo, ¿la colinealidad es un problema potencial?Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar una gráfica de probabilidad normal de los residuales. ¿Parece haber algún problema con la hipótesis de normalidad?Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar e interpretar una gráfica de los residuales en función de la respuesta predicha.Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?.Trazar las gráficas de los residuales en función de cada una de las variables regresoras. ¿Implican esas gráficas que se especificó en forma correcta el regresor?.Ejercicio 4: Explica lo siguiente.¿Qué supuestos del modelo de regresión lineal múltiple deben verificarse?¿Qué supuestos del modelo de regresión lineal múltiple deben verificarse?¿Cómo se interpretan los intervalos de confianza? Si construimos un intervalo de confianza del 95% para un coeficiente \\(\\beta_j\\), ¿cuál sería la lectura correcta o interpretación correcta sobre este intervalo?¿Cómo se interpretan los intervalos de confianza? Si construimos un intervalo de confianza del 95% para un coeficiente \\(\\beta_j\\), ¿cuál sería la lectura correcta o interpretación correcta sobre este intervalo?Describe los métodos de selección de variables y sus ventajas y desventajas:Describe los métodos de selección de variables y sus ventajas y desventajas:Selección hacia adelante (forward)Selección hacia adelante (forward)Selección hacia atrás (backward)Selección hacia atrás (backward)selección por pasos (stepwise) y/o mejor subconjunto (best subset)selección por pasos (stepwise) y/o mejor subconjunto (best subset)Explica cómo se utilizan para elegir el modelo final.Ejercicio 5: Para los datos del ejercicio 1 de la liga de Futbol. Realizar el ejercicio en R y Python.Usar el algoritmo de selección hacia adelante para seleccionar un modelo de regresión.Usar el algoritmo de selección hacia adelante para seleccionar un modelo de regresión.Usar el algoritmo de selección hacia atrás para seleccionar un modelo de regresión.Usar el algoritmo de selección hacia atrás para seleccionar un modelo de regresión.Usar el algoritmo de regresión por pasos para seleccionar un modelo de regresión.Usar el algoritmo de regresión por pasos para seleccionar un modelo de regresión.Comenta los modelos finales en cada uno de los casos anteriores. ¿Cuál tiene más sentido? ¿Cuál modelo usarían?Comenta los modelos finales en cada uno de los casos anteriores. ¿Cuál tiene más sentido? ¿Cuál modelo usarían?","code":""},{"path":"regresión-múltiple.html","id":"validación-de-supuestos","chapter":"Sección 2 Regresión múltiple","heading":"2.7 Validación de Supuestos","text":"Ejemplo: Se llevó cabo un conjunto de ensayos experimentales con un horno para determinar una forma de predecir el tiempo de cocción, \\(y\\), diferentes niveles de ancho del horno, \\(x_1\\), y diferentes temperaturas, \\(x_2\\). Se registraron los siguientes datos:Table 2.1: Factores que influyen en el tiempo de coccion segun diferentes niveles de ancho del horno y diferentes temperaturasVariable dependiente \\(y =\\) tiempo de cocciónVariable dependiente \\(y =\\) tiempo de cocciónVariable independiente \\(x_1 =\\) ancho del hornoVariable independiente \\(x_1 =\\) ancho del hornoVariable independiente \\(x_2 =\\) diferentes temperaturasVariable independiente \\(x_2 =\\) diferentes temperaturasVamos visualizar los datos:Ahora, vamos analizar algunos de los supuestos.","code":"\nyp <-c(6.40, 15.05, 18.75, 30.25, 44.85, 48.85, 51.55, 61.50, 100.44, 111.42)\nx1 <-c(1.32, 2.69, 3.56, 4.41, 5.35, 6.20, 7.12, 8.87, 9.80, 10.65)\nx2 <-c(1.15, 3.40, 4.10, 8.75, 14.82, 15.15, 15.32, 18.18, 35.19, 40.40)\ndatos<-data.frame(yp, x1, x2)\nkable(datos, caption = \"Factores que influyen en el tiempo de coccion segun diferentes niveles de ancho del horno y diferentes temperaturas\")\ng1 <- ggplot(data = datos, mapping = aes(x = x1, y = yp)) +\n  geom_point(color = \"forestgreen\", size = 2) +\n  labs(title  =  'yp ~ x1', x  =  'x1') +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\ng2 <- ggplot(data = datos, mapping = aes(x = x2, y = yp)) +\n  geom_point(color = \"orange\", size = 2) +\n  labs(title  =  'yp ~ x2', x  =  'x2') +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ng1+g2## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"regresión-múltiple.html","id":"multicolinealidad","chapter":"Sección 2 Regresión múltiple","heading":"2.7.1 Multicolinealidad","text":"Hay una fuerte correlación entre las variables, lo cual es un problema dado que las variables deberían ser independientes.Vamos construir dos modelos.El intercepto parece ser significativo. Vamos construir un segundo modelo usando solo \\(x_2\\) que parece ser más significativa.El ANOVA nos puede ayudar ver cual modelo es más significativo. Se usan las hipótesis siguientes:\\(H_0:\\) Las variables que eliminamos tienen significancia.\\(H_1:\\) Las variables son significativas.Si el nuevo modelo es una mejora del modelo original, entonces podemos rechazar \\(H_0\\). Si ese es el caso, significa que esas variables fueron significativas; por lo tanto rechazamos \\(H_0\\).Como el p-valor es muy pequeño, menor al valor de significancia 0.05, entonces rechazamos la hipótesis nula, lo que nos dice que el segundo modelo es una mejora del primero.Como desde el inicio vimos que el coeficiente correspondiente \\(\\beta_0\\) era significativo, vamos eliminarlo.","code":"\nvariables <- data.frame(x1,x2)\nm_cor <- cor(variables,method = \"pearson\")\nm_cor##           x1        x2\n## x1 1.0000000 0.9375592\n## x2 0.9375592 1.0000000\nlibrary(corrplot)## corrplot 0.95 loaded\ncorrplot(m_cor)\nmodelo1 <- lm(formula = yp ~ x1 + x2, data = datos)\n\nsummary(modelo1)## \n## Call:\n## lm(formula = yp ~ x1 + x2, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.8475 -0.3438  0.0043  0.2554  1.1578 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.57723    0.59865   0.964    0.367    \n## x1           2.70957    0.19935  13.592 2.75e-06 ***\n## x2           2.05033    0.04743  43.227 9.26e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6481 on 7 degrees of freedom\n## Multiple R-squared:  0.9997, Adjusted R-squared:  0.9997 \n## F-statistic: 1.304e+04 on 2 and 7 DF,  p-value: 3.166e-13\nmodelo2 <- lm(formula = yp ~ x2, data = datos)\n\nsummary(modelo2)## \n## Call:\n## lm(formula = yp ~ x2, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.0226 -1.7338 -0.3497  1.0695  5.8668 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  7.36967    1.61355   4.567  0.00183 ** \n## x2           2.65476    0.08077  32.869 8.01e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.173 on 8 degrees of freedom\n## Multiple R-squared:  0.9926, Adjusted R-squared:  0.9917 \n## F-statistic:  1080 on 1 and 8 DF,  p-value: 8.005e-10\nanova(modelo1, modelo2)## Analysis of Variance Table\n## \n## Model 1: yp ~ x1 + x2\n## Model 2: yp ~ x2\n##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n## 1      7  2.940                                  \n## 2      8 80.532 -1   -77.592 184.74 2.745e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodelo3 <- lm(formula = yp ~ x1 + x2 -1, data = datos)\n\nsummary(modelo3)## \n## Call:\n## lm(formula = yp ~ x1 + x2 - 1, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.8103 -0.3698  0.1963  0.3955  1.1807 \n## \n## Coefficients:\n##    Estimate Std. Error t value Pr(>|t|)    \n## x1  2.87003    0.10927   26.27 4.74e-09 ***\n## x2  2.02140    0.03657   55.28 1.27e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6452 on 8 degrees of freedom\n## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 \n## F-statistic: 4.188e+04 on 2 and 8 DF,  p-value: < 2.2e-16"},{"path":"regresión-múltiple.html","id":"normalidad-en-los-residuales","chapter":"Sección 2 Regresión múltiple","heading":"2.7.2 Normalidad en los residuales","text":"Recordemos que los residuos se calculan como la diferencia entre el valor observado \\((y)\\) y el valor predicho \\((\\hat{y})\\) para cada punto de datos, es decir:\\(e = y - \\hat{y}\\)Vamos hacer un plot de los residuales.Otra forma de obtener este plot es la siguiente.El test de Shapiro-Wilks plantea la hipótesis nula que una muestra proviene de una distribución normal. Eligimos un nivel de significanza, por ejemplo \\(0.05\\), y tenemos una hipótesis alternativa que sostiene que la distribución es normal. Tenemos entonces lo siguiente:\\(H_0:\\) La distribución es normal.\\(H_1:\\) La distribución es normal.Como el p-valor es más grande que el valor de significancia, podemos rechazar la hipótesis nula, por lo tanto los residuales siguen una distribución normal.","code":"\nresiduales = modelo3$residuals\n\n## Q-Q plot\nqqnorm(residuales)\nqqline(residuales)\nplot(modelo3)\nshapiro.test(residuales)## \n##  Shapiro-Wilk normality test\n## \n## data:  residuales\n## W = 0.95058, p-value = 0.6754"},{"path":"regresión-múltiple.html","id":"homocedasticidad","chapter":"Sección 2 Regresión múltiple","heading":"2.7.3 Homocedasticidad","text":"Homocedasticidad = varianza constanteCorrecto: Si los residuales están dispersos uniformemente lo largo de todos los valores predichos.Correcto: Si los residuales están dispersos uniformemente lo largo de todos los valores predichos.Problema: Si vemos un patrón de embudo (residuales pequeños para predichos bajos y grandes para predichos altos, o viceversa). Esto indica heterocedasticidad.Problema: Si vemos un patrón de embudo (residuales pequeños para predichos bajos y grandes para predichos altos, o viceversa). Esto indica heterocedasticidad.Linealidad y errores independientes: Si se notan curvas, arcos o patrones sistemáticos, podría indicar que:La relación es estrictamente lineal.La relación es estrictamente lineal.Falta alguna variable importante en el modelo.Falta alguna variable importante en el modelo.O hay correlación entre errores.O hay correlación entre errores.Una forma de verlo es con el plot de residuales vs valores predichos.En R, existe la función bptest(), que es el test de Breusch-Pagan para la heterocedasticidad. Esta función toma como entrada un modelo de regresión y devuelve el resultado de la prueba de hipótesis para la homocedasticidad de los residuos.\\(H_O:\\) los residuos tienen varianza constante (homocedasticidad)\\(H_1\\): hay heterocedasticidad en los residuosEl resultado incluye el valor del estadístico de prueba (el valor de la prueba de Breusch-Pagan), el p-valor y el número de grados de libertad. Si el p-valor es menor que el nivel de significancia elegido, se rechaza la hipótesis nula de homocedasticidad y se concluye que hay heterocedasticidad en los residuos.Entonces como el p-valor es menor al valor de significancia \\(0.05\\), rechazamos la hipótesis nula y podemos decir que existe heterocedasticidad en los residuales.La heterocedasticidad es un problema porque la regresión de mínimos cuadrados ordinarios asume que todos los residuales se extraen de una población que tiene una varianza constante (homocedasticidad).Una forma de corregirlo es haciendo una transformación de los datos. Vamos transformar la variable \\(x_1\\).\nNota: Estas transformaciones deben de justificarse y explicar el porque.Si realizamos la prueba de la homocedasticidad.Vemos que ahora el p-valor es más grande que el valor de significancia, lo cual nos indica que podemos rechazar \\(H_0\\), es decir ahora si podemos asumir que hay homocedasticidad.En el modelo final, tendríamos \\(\\beta_0=0\\), \\(\\beta_1*=\\) 7.5272361 y \\(\\beta_2=\\) 2.3401283.Otras dos pruebas que se pueden usar son fligner.test y leveneTest.","code":"\npar(mfrow = c(2, 2))\nplot(modelo3)\nlibrary(lmtest)## Cargando paquete requerido: zoo## \n## Adjuntando el paquete: 'zoo'## The following objects are masked from 'package:base':\n## \n##     as.Date, as.Date.numeric\nbptest(modelo3)## \n##  studentized Breusch-Pagan test\n## \n## data:  modelo3\n## BP = 5.7517, df = 1, p-value = 0.01647\nmodelo4 <- lm(formula = yp ~ log(x1) + x2 -1, data = datos)\n\nsummary(modelo4)## \n## Call:\n## lm(formula = yp ~ log(x1) + x2 - 1, data = datos)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.4546 -0.7961 -0.3458  0.9207  2.5270 \n## \n## Coefficients:\n##         Estimate Std. Error t value Pr(>|t|)    \n## log(x1)  7.52724    0.72213   10.42 6.22e-06 ***\n## x2       2.34013    0.06306   37.11 3.05e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.578 on 8 degrees of freedom\n## Multiple R-squared:  0.9994, Adjusted R-squared:  0.9993 \n## F-statistic:  6997 on 2 and 8 DF,  p-value: 1.065e-13\nbptest(modelo4)## \n##  studentized Breusch-Pagan test\n## \n## data:  modelo4\n## BP = 0.13878, df = 1, p-value = 0.7095\npar(mfrow = c(2, 2))\nplot(modelo4)"},{"path":"regresión-múltiple.html","id":"no-autocorrelación","chapter":"Sección 2 Regresión múltiple","heading":"2.7.4 No autocorrelación","text":"Una forma de revisar este supuesto es con el test de Durbin-Watson. Las hipótesis que se tienen son:\\(H_0:\\) hay autocorrelación en los errores (los residuales son independientes).\\(H_1:\\) Hay autocorrelación en los errores (generalmente, autocorrelación positiva de primer orden).El estadístico DW toma valores entre 0 y 4:\\(DW\\) aproximadamente 2, entonces hay autocorrelación (se cumple el supuesto).\\(DW\\) aproximadamente 2, entonces hay autocorrelación (se cumple el supuesto).\\(DW < 2\\), entonces indica autocorrelación positiva (los errores tienden repetirse).\\(DW < 2\\), entonces indica autocorrelación positiva (los errores tienden repetirse).\\(DW > 2\\), entonces indica autocorrelación negativa (los errores tienden alternar signo).\\(DW > 2\\), entonces indica autocorrelación negativa (los errores tienden alternar signo).Vamos predecir por último un valor. Para \\(2.10\\) de ancho del horno y una temperatura de \\(3.10\\) , ¿cuánto seria el tiempo de cocción?","code":"\ndwtest(modelo4)## \n##  Durbin-Watson test\n## \n## data:  modelo4\n## DW = 1.036, p-value = 0.02046\n## alternative hypothesis: true autocorrelation is greater than 0\nnuevo.dato <- data.frame(x1 = 2.10, x2 = 3.10)\n\nprediccion <- predict(modelo4, newdata = nuevo.dato)\n\npaste(\"La cantidad estimada de tiempo de coccion es:\", round(prediccion, 2))## [1] \"La cantidad estimada de tiempo de coccion es: 12.84\""},{"path":"regresión-múltiple.html","id":"ejercicios-1","chapter":"Sección 2 Regresión múltiple","heading":"2.7.5 Ejercicios","text":"Ejercicio 1: Para los datos de Datarium marketing, analiza los supuestos. Explica tus resultados y sube tus respuestas github.","code":""},{"path":"análisis-de-componentes-principales.html","id":"análisis-de-componentes-principales","chapter":"Sección 3 Análisis de Componentes Principales","heading":"Sección 3 Análisis de Componentes Principales","text":"","code":""},{"path":"análisis-factorial.html","id":"análisis-factorial","chapter":"Sección 4 Análisis Factorial","heading":"Sección 4 Análisis Factorial","text":"","code":""},{"path":"análisis-de-conglomerados.html","id":"análisis-de-conglomerados","chapter":"Sección 5 Análisis de Conglomerados","heading":"Sección 5 Análisis de Conglomerados","text":"","code":""},{"path":"análisis-de-discriminante.html","id":"análisis-de-discriminante","chapter":"Sección 6 Análisis de Discriminante","heading":"Sección 6 Análisis de Discriminante","text":"","code":""},{"path":"apéndices.html","id":"apéndices","chapter":"Sección 7 Apéndices","heading":"Sección 7 Apéndices","text":"","code":""},{"path":"apéndices.html","id":"introducción-a-r","chapter":"Sección 7 Apéndices","heading":"7.1 Introducción a R","text":"Tutorial de RMarkdown: LinkTutorial de RMarkdown: LinkTutorial Manejo de Proyectos: LinkTutorial Manejo de Proyectos: Link","code":""},{"path":"apéndices.html","id":"git-github","chapter":"Sección 7 Apéndices","heading":"7.2 Git + Github","text":"Conectar R con Git y Github: Link","code":""},{"path":"apéndices.html","id":"gráficas-multivariadas","chapter":"Sección 7 Apéndices","heading":"7.3 Gráficas Multivariadas","text":"","code":""},{"path":"apéndices.html","id":"escalas-de-medición","chapter":"Sección 7 Apéndices","heading":"7.4 Escalas de Medición","text":"","code":""},{"path":"apéndices.html","id":"valores-faltantes","chapter":"Sección 7 Apéndices","heading":"7.5 Valores Faltantes","text":"","code":""}]
